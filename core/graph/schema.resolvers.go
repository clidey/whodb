/*
 * Copyright 2025 Clidey, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package graph

// This file will be automatically regenerated based on the schema, any resolver
// implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen version v0.17.84

import (
	"context"
	"errors"
	"fmt"
	"strings"

	"github.com/clidey/whodb/core/graph/model"
	"github.com/clidey/whodb/core/src"
	"github.com/clidey/whodb/core/src/analytics"
	"github.com/clidey/whodb/core/src/auth"
	"github.com/clidey/whodb/core/src/common"
	"github.com/clidey/whodb/core/src/engine"
	"github.com/clidey/whodb/core/src/env"
	"github.com/clidey/whodb/core/src/llm"
	"github.com/clidey/whodb/core/src/log"
	gorm_plugin "github.com/clidey/whodb/core/src/plugins/gorm"
	"github.com/clidey/whodb/core/src/settings"
	"golang.org/x/sync/errgroup"
	"gorm.io/gorm"
)

// Login is the resolver for the Login field.
func (r *mutationResolver) Login(ctx context.Context, credentials model.LoginCredentials) (*model.StatusResponse, error) {
	var advanced []engine.Record
	for _, recordInput := range credentials.Advanced {
		advanced = append(advanced, engine.Record{
			Key:   recordInput.Key,
			Value: recordInput.Value,
		})
	}

	hasProfileID := credentials.ID != nil && strings.TrimSpace(*credentials.ID) != ""
	identity := strings.TrimSpace(analytics.MetadataFromContext(ctx).DistinctID)
	hasIdentity := identity != "" && identity != "disabled"

	if hasIdentity {
		analytics.CaptureWithDistinctID(ctx, identity, "login.attempt", map[string]any{
			"database_type":      credentials.Type,
			"profile_id_present": hasProfileID,
		})
	}

	if !src.MainEngine.Choose(engine.DatabaseType(credentials.Type)).IsAvailable(&engine.PluginConfig{
		Credentials: &engine.Credentials{
			Type:     credentials.Type,
			Hostname: credentials.Hostname,
			Username: credentials.Username,
			Password: credentials.Password,
			Database: credentials.Database,
			Advanced: advanced,
		},
	}) {
		log.LogFields(log.Fields{
			"type":     credentials.Type,
			"hostname": credentials.Hostname,
			"username": credentials.Username,
			"database": credentials.Database,
		}).Error("Database connection failed during login - credentials unauthorized")

		if hasIdentity {
			analytics.CaptureWithDistinctID(ctx, identity, "login.denied", map[string]any{
				"database_type":      credentials.Type,
				"profile_id_present": hasProfileID,
			})
		}
		return nil, errors.New("unauthorized")
	}

	resp, err := auth.Login(ctx, &credentials)
	if err != nil {
		if hasIdentity {
			analytics.CaptureError(ctx, "login.execute", err, map[string]any{
				"database_type":      credentials.Type,
				"profile_id_present": hasProfileID,
			})
		}
		return nil, err
	}

	if hasIdentity {
		traits := map[string]any{
			"profile_id_present": hasProfileID,
		}
		if hashedHost := analytics.HashIdentifier(credentials.Hostname); hashedHost != "" {
			traits["hostname_hash"] = hashedHost
		}
		if hashedDatabase := analytics.HashIdentifier(credentials.Database); hashedDatabase != "" {
			traits["database_hash"] = hashedDatabase
		}

		analytics.IdentifyWithDistinctID(ctx, identity, traits)
		analytics.CaptureWithDistinctID(ctx, identity, "login.success", map[string]any{
			"database_type":      credentials.Type,
			"profile_id_present": hasProfileID,
		})
	}

	return resp, nil
}

// LoginWithProfile is the resolver for the LoginWithProfile field.
func (r *mutationResolver) LoginWithProfile(ctx context.Context, profile model.LoginProfileInput) (*model.StatusResponse, error) {
	profiles := src.GetLoginProfiles()
	for i, loginProfile := range profiles {
		profileId := src.GetLoginProfileId(i, loginProfile)
		if profile.ID == profileId {

			resolved := src.GetLoginCredentials(loginProfile)
			credentials := &model.LoginCredentials{
				ID:       &profile.ID,
				Type:     resolved.Type,
				Hostname: resolved.Hostname,
				Username: resolved.Username,
				Password: resolved.Password,
				Database: resolved.Database,
				Advanced: func() []*model.RecordInput {
					var out []*model.RecordInput
					for _, rec := range resolved.Advanced {
						out = append(out, &model.RecordInput{Key: rec.Key, Value: rec.Value})
					}
					return out
				}(),
			}
			if profile.Database != nil && *profile.Database != "" {
				credentials.Database = *profile.Database
				resolved.Database = credentials.Database
			}

			identity := strings.TrimSpace(analytics.MetadataFromContext(ctx).DistinctID)
			hasIdentity := identity != "" && identity != "disabled"

			if hasIdentity {
				analytics.CaptureWithDistinctID(ctx, identity, "login_with_profile.attempt", map[string]any{
					"database_type":  loginProfile.Type,
					"profile_source": loginProfile.Source,
				})
			}

			if !src.MainEngine.Choose(engine.DatabaseType(loginProfile.Type)).IsAvailable(&engine.PluginConfig{
				Credentials: resolved,
			}) {
				log.LogFields(log.Fields{
					"profile_id": profile.ID,
					"type":       loginProfile.Type,
				}).Error("Database connection failed for login profile - credentials unauthorized")

				if hasIdentity {
					analytics.CaptureWithDistinctID(ctx, identity, "login_with_profile.denied", map[string]any{
						"database_type":  loginProfile.Type,
						"profile_source": loginProfile.Source,
					})
				}
				return nil, errors.New("unauthorized")
			}

			resp, err := auth.Login(ctx, credentials)
			if err != nil {
				if hasIdentity {
					analytics.CaptureError(ctx, "login_with_profile.execute", err, map[string]any{
						"database_type":  loginProfile.Type,
						"profile_source": loginProfile.Source,
					})
				}
				return nil, err
			}

			if hasIdentity {
				traits := map[string]any{
					"profile_source": loginProfile.Source,
					"saved_profile":  true,
				}
				if hashedHost := analytics.HashIdentifier(credentials.Hostname); hashedHost != "" {
					traits["hostname_hash"] = hashedHost
				}
				if hashedDatabase := analytics.HashIdentifier(credentials.Database); hashedDatabase != "" {
					traits["database_hash"] = hashedDatabase
				}

				analytics.IdentifyWithDistinctID(ctx, identity, traits)
				analytics.CaptureWithDistinctID(ctx, identity, "login_with_profile.success", map[string]any{
					"database_type":  loginProfile.Type,
					"profile_source": loginProfile.Source,
				})
			}

			return resp, nil
		}
	}
	log.LogFields(log.Fields{
		"profile_id": profile.ID,
	}).Error("Login profile not found or not authorized")
	return nil, errors.New("login profile does not exist or is not authorized")
}

// Logout is the resolver for the Logout field.
func (r *mutationResolver) Logout(ctx context.Context) (*model.StatusResponse, error) {
	creds := auth.GetCredentials(ctx)
	identity := strings.TrimSpace(analytics.MetadataFromContext(ctx).DistinctID)
	hasIdentity := identity != "" && identity != "disabled"
	hasProfile := false
	dbType := ""
	if creds != nil {
		hasProfile = creds.Id != nil && strings.TrimSpace(*creds.Id) != ""
		dbType = creds.Type
	}

	if hasIdentity {
		analytics.CaptureWithDistinctID(ctx, identity, "logout.attempt", map[string]any{
			"database_type":      dbType,
			"profile_id_present": hasProfile,
		})
	}

	resp, err := auth.Logout(ctx)
	if err != nil {
		if hasIdentity {
			analytics.CaptureError(ctx, "logout.execute", err, map[string]any{
				"database_type":      dbType,
				"profile_id_present": hasProfile,
			})
		}
		return nil, err
	}

	if hasIdentity {
		analytics.CaptureWithDistinctID(ctx, identity, "logout.success", map[string]any{
			"database_type":      dbType,
			"profile_id_present": hasProfile,
		})
	}

	return resp, nil
}

// UpdateSettings is the resolver for the UpdateSettings field.
func (r *mutationResolver) UpdateSettings(ctx context.Context, newSettings model.SettingsConfigInput) (*model.StatusResponse, error) {
	var fields []settings.ISettingsField

	if newSettings.MetricsEnabled != nil {
		metricsEnabled := common.StrPtrToBool(newSettings.MetricsEnabled)
		fields = append(fields, settings.MetricsEnabledField(metricsEnabled))

		analytics.TrackMutation(ctx, "UpdateSettings.metrics", map[string]any{
			"metrics_enabled": metricsEnabled,
		})
	}

	updated := settings.UpdateSettings(fields...)
	return &model.StatusResponse{
		Status: updated,
	}, nil
}

// AddStorageUnit is the resolver for the AddStorageUnit field.
func (r *mutationResolver) AddStorageUnit(ctx context.Context, schema string, storageUnit string, fields []*model.RecordInput) (*model.StatusResponse, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type
	fieldsMap := []engine.Record{}
	for _, field := range fields {
		extraFields := map[string]string{}
		for _, extraField := range field.Extra {
			extraFields[extraField.Key] = extraField.Value
		}
		fieldsMap = append(fieldsMap, engine.Record{
			Key:   field.Key,
			Value: field.Value,
			Extra: extraFields,
		})
	}
	status, err := plugin.AddStorageUnit(config, schema, storageUnit, fieldsMap)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "AddStorageUnit",
			"schema":        schema,
			"storage_unit":  storageUnit,
			"database_type": typeArg,
		}).WithError(err).Error("Database operation failed")
		analytics.CaptureError(ctx, "AddStorageUnit", err, map[string]any{
			"database_type": typeArg,
			"schema_hash":   analytics.HashIdentifier(schema),
			"storage_hash":  analytics.HashIdentifier(storageUnit),
		})
		return nil, err
	}

	analytics.TrackMutation(ctx, "AddStorageUnit", map[string]any{
		"database_type": typeArg,
		"schema_hash":   analytics.HashIdentifier(schema),
		"storage_hash":  analytics.HashIdentifier(storageUnit),
		"field_count":   len(fields),
	})

	return &model.StatusResponse{
		Status: status,
	}, nil
}

// UpdateStorageUnit is the resolver for the UpdateStorageUnit field.
func (r *mutationResolver) UpdateStorageUnit(ctx context.Context, schema string, storageUnit string, values []*model.RecordInput, updatedColumns []string) (*model.StatusResponse, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type

	if err := ValidateStorageUnit(plugin, config, schema, storageUnit); err != nil {
		return nil, err
	}

	valuesMap := map[string]string{}
	for _, value := range values {
		valuesMap[value.Key] = value.Value
	}
	status, err := plugin.UpdateStorageUnit(config, schema, storageUnit, valuesMap, updatedColumns)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":       "UpdateStorageUnit",
			"schema":          schema,
			"storage_unit":    storageUnit,
			"database_type":   typeArg,
			"updated_columns": len(updatedColumns),
		}).WithError(err).Error("Database operation failed")
		analytics.CaptureError(ctx, "UpdateStorageUnit", err, map[string]any{
			"database_type":   typeArg,
			"schema_hash":     analytics.HashIdentifier(schema),
			"storage_hash":    analytics.HashIdentifier(storageUnit),
			"updated_columns": len(updatedColumns),
			"values_supplied": len(values),
		})
		return nil, err
	}

	analytics.TrackMutation(ctx, "UpdateStorageUnit", map[string]any{
		"database_type":   typeArg,
		"schema_hash":     analytics.HashIdentifier(schema),
		"storage_hash":    analytics.HashIdentifier(storageUnit),
		"updated_columns": len(updatedColumns),
		"values_supplied": len(values),
	})

	return &model.StatusResponse{
		Status: status,
	}, nil
}

// AddRow is the resolver for the AddRow field.
func (r *mutationResolver) AddRow(ctx context.Context, schema string, storageUnit string, values []*model.RecordInput) (*model.StatusResponse, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type

	if err := ValidateStorageUnit(plugin, config, schema, storageUnit); err != nil {
		return nil, err
	}

	log.LogFields(log.Fields{
		"operation":     "AddRow-Resolver",
		"schema":        schema,
		"storage_unit":  storageUnit,
		"database_type": typeArg,
		"values_count":  len(values),
	}).Debug("AddRow resolver called")

	valuesRecords := []engine.Record{}
	for _, field := range values {
		extraFields := map[string]string{}
		for _, extraField := range field.Extra {
			extraFields[extraField.Key] = extraField.Value
		}
		valuesRecords = append(valuesRecords, engine.Record{
			Key:   field.Key,
			Value: field.Value,
			Extra: extraFields,
		})
	}

	status, err := plugin.AddRow(config, schema, storageUnit, valuesRecords)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "AddRow",
			"schema":        schema,
			"storage_unit":  storageUnit,
			"database_type": typeArg,
		}).WithError(err).Error("Database operation failed")
		analytics.CaptureError(ctx, "AddRow", err, map[string]any{
			"database_type": typeArg,
			"schema_hash":   analytics.HashIdentifier(schema),
			"storage_hash":  analytics.HashIdentifier(storageUnit),
			"value_count":   len(values),
		})
		return nil, err
	}

	analytics.TrackMutation(ctx, "AddRow", map[string]any{
		"database_type": typeArg,
		"schema_hash":   analytics.HashIdentifier(schema),
		"storage_hash":  analytics.HashIdentifier(storageUnit),
		"value_count":   len(values),
	})

	return &model.StatusResponse{
		Status: status,
	}, nil
}

// DeleteRow is the resolver for the DeleteRow field.
func (r *mutationResolver) DeleteRow(ctx context.Context, schema string, storageUnit string, values []*model.RecordInput) (*model.StatusResponse, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type

	if err := ValidateStorageUnit(plugin, config, schema, storageUnit); err != nil {
		return nil, err
	}

	valuesMap := map[string]string{}
	for _, value := range values {
		valuesMap[value.Key] = value.Value
	}
	status, err := plugin.DeleteRow(config, schema, storageUnit, valuesMap)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "DeleteRow",
			"schema":        schema,
			"storage_unit":  storageUnit,
			"database_type": typeArg,
		}).WithError(err).Error("Database operation failed")
		analytics.CaptureError(ctx, "DeleteRow", err, map[string]any{
			"database_type": typeArg,
			"schema_hash":   analytics.HashIdentifier(schema),
			"storage_hash":  analytics.HashIdentifier(storageUnit),
			"value_count":   len(values),
		})
		return nil, err
	}

	analytics.TrackMutation(ctx, "DeleteRow", map[string]any{
		"database_type": typeArg,
		"schema_hash":   analytics.HashIdentifier(schema),
		"storage_hash":  analytics.HashIdentifier(storageUnit),
		"value_count":   len(values),
	})

	return &model.StatusResponse{
		Status: status,
	}, nil
}

// GenerateMockData is the resolver for the GenerateMockData field.
func (r *mutationResolver) GenerateMockData(ctx context.Context, input model.MockDataGenerationInput) (*model.MockDataGenerationStatus, error) {
	log.Logger.WithField("schema", input.Schema).WithField("table", input.StorageUnit).WithField("rowCount", input.RowCount).WithField("overwrite", input.OverwriteExisting).Info("Starting mock data generation")

	maxRowLimit := env.GetMockDataGenerationMaxRowCount()
	if input.RowCount > maxRowLimit {
		log.Logger.WithField("requested", input.RowCount).WithField("max", maxRowLimit).Error("Row count exceeds maximum limit")
		return nil, fmt.Errorf("row count exceeds maximum limit of %d", maxRowLimit)
	}

	if !env.IsMockDataGenerationAllowed(input.StorageUnit) {
		log.Logger.WithField("table", input.StorageUnit).Error("Mock data generation not allowed for table")
		return nil, errors.New("mock data generation is not allowed for this table")
	}

	plugin, config := GetPluginForContext(ctx)

	rowsResult, err := plugin.GetRows(config, input.Schema, input.StorageUnit, nil, []*model.SortCondition{}, 1, 0)
	if err != nil {
		log.Logger.WithError(err).Error("Failed to get table schema")
		return nil, fmt.Errorf("failed to get table schema: %w", err)
	}
	log.Logger.WithField("columnCount", len(rowsResult.Columns)).Debug("Got table columns")

	constraints, err := plugin.GetColumnConstraints(config, input.Schema, input.StorageUnit)
	if err != nil {
		log.Logger.WithError(err).Warn("Failed to get column constraints, using empty constraints")
		constraints = make(map[string]map[string]any)
	}
	log.Logger.WithField("constraintCount", len(constraints)).Debug("Got column constraints")

	// Generate extra rows as buffer for potential constraint violations
	// We'll generate 50% more rows as buffer, capped at requested + 100
	generator := src.NewMockDataGenerator()
	bufferSize := min(input.RowCount/2, 100)
	maxGenerationAttempts := input.RowCount + bufferSize

	generatedRowsBuffer := make([][]engine.Record, 0, maxGenerationAttempts)
	attemptsCount := 0
	maxTotalAttempts := maxGenerationAttempts * 10 // Overall safety limit to prevent infinite loops

	// Keep generating until we have enough rows or hit the safety limit
	log.Logger.WithField("targetRows", maxGenerationAttempts).Info("Starting row generation")
	for len(generatedRowsBuffer) < maxGenerationAttempts && attemptsCount < maxTotalAttempts {
		attemptsCount++
		rowData, genErr := generator.GenerateRowDataWithConstraints(rowsResult.Columns, constraints)
		if genErr != nil {
			log.Logger.WithError(genErr).WithField("attempt", attemptsCount).Debug("Failed to generate row")
			continue
		}
		generatedRowsBuffer = append(generatedRowsBuffer, rowData)
	}

	log.Logger.WithField("generatedRows", len(generatedRowsBuffer)).WithField("attempts", attemptsCount).Info("Completed row generation buffer")

	if len(generatedRowsBuffer) == 0 {
		log.Logger.Error("Failed to generate any valid rows")
		return nil, errors.New("failed to generate any valid rows after multiple attempts")
	}

	gormPlugin, isGormPlugin := plugin.PluginFunctions.(*gorm_plugin.GormPlugin)

	generatedRows := 0
	if isGormPlugin {
		// SQL databases use transactions for atomicity
		err = gormPlugin.ExecuteInTransaction(config, func(tx *gorm.DB) error {
			if input.OverwriteExisting {
				if err := gormPlugin.ClearTableDataInTx(tx, input.Schema, input.StorageUnit); err != nil {
					return fmt.Errorf("failed to clear existing data: %w", err)
				}
			}

			// keep trying from buffer until we hit the exact count
			successfulRows := 0
			bufferIndex := 0
			retryWithDefaults := false

			for successfulRows < input.RowCount && bufferIndex < len(generatedRowsBuffer) {
				rowData := generatedRowsBuffer[bufferIndex]
				bufferIndex++

				if err := gormPlugin.AddRowInTx(tx, input.Schema, input.StorageUnit, rowData); err != nil {
					errStr := err.Error()
					log.Logger.WithError(err).WithField("rowIndex", bufferIndex).WithField("successfulSoFar", successfulRows).Error("Failed to insert row")
					// If it's a constraint error, try with default values
					if strings.Contains(errStr, "constraint") || strings.Contains(errStr, "CHECK") || strings.Contains(errStr, "check") {
						log.Logger.Error("Constraint violation detected, trying with default values")
						// Try once with defaults
						if !retryWithDefaults {
							defaultRow := generator.GenerateRowWithDefaults(rowsResult.Columns)
							if retryErr := gormPlugin.AddRowInTx(tx, input.Schema, input.StorageUnit, defaultRow); retryErr == nil {
								log.Logger.Info("Successfully inserted row with default values")
								successfulRows++
								retryWithDefaults = false
								continue
							} else {
								log.Logger.WithError(retryErr).Error("Failed to insert row with default values")
							}
							retryWithDefaults = true // Mark that we've tried defaults
						}
						// Skip this row and try next from buffer
						continue
					}
					// For non-constraint errors in overwrite mode, fail the transaction
					if input.OverwriteExisting {
						log.Logger.WithError(err).Error("Non-constraint error in overwrite mode, failing transaction")
						return fmt.Errorf("failed to insert row: %w", err)
					}
					log.Logger.WithError(err).Warn("Non-constraint error in append mode, skipping row")
					// For append mode, skip and continue
					continue
				}
				successfulRows++
				retryWithDefaults = false
				if successfulRows%10 == 0 {
					log.Logger.WithField("progress", successfulRows).WithField("target", input.RowCount).Debug("Mock data insertion progress")
				}
			}

			// If we couldn't generate exactly the requested amount, try generating more on the fly
			log.Logger.WithField("successfulRows", successfulRows).WithField("target", input.RowCount).Debug("Checking if additional generation needed")
			additionalAttempts := 0
			maxAdditionalAttempts := input.RowCount * 5 // Safety limit

			for successfulRows < input.RowCount && additionalAttempts < maxAdditionalAttempts {
				additionalAttempts++

				// Generate a new row
				newRow, genErr := generator.GenerateRowDataWithConstraints(rowsResult.Columns, constraints)
				if genErr != nil {
					continue
				}

				if err := gormPlugin.AddRowInTx(tx, input.Schema, input.StorageUnit, newRow); err != nil {
					defaultRow := generator.GenerateRowWithDefaults(rowsResult.Columns)
					if retryErr := gormPlugin.AddRowInTx(tx, input.Schema, input.StorageUnit, defaultRow); retryErr == nil {
						successfulRows++
					}
					continue
				}
				successfulRows++
			}

			generatedRows = successfulRows
			log.Logger.WithField("generatedRows", generatedRows).WithField("target", input.RowCount).Info("Completed mock data insertion in transaction")

			// If overwriting and we couldn't generate enough rows, fail the transaction
			if input.OverwriteExisting && generatedRows == 0 {
				log.Logger.Error("Failed to generate any valid rows in overwrite mode")
				return errors.New("failed to generate any valid rows - existing data preserved")
			}

			return nil
		})
	} else {
		// NoSQL: no transaction support, operations may partially succeed
		if input.OverwriteExisting {
			if _, err := plugin.ClearTableData(config, input.Schema, input.StorageUnit); err != nil {
				return nil, fmt.Errorf("failed to clear existing data: %w", err)
			}
		}

		bufferIndex := 0
		for generatedRows < input.RowCount && bufferIndex < len(generatedRowsBuffer) {
			rowData := generatedRowsBuffer[bufferIndex]
			bufferIndex++

			success, addErr := plugin.AddRow(config, input.Schema, input.StorageUnit, rowData)
			if addErr != nil || !success {
				defaultRow := generator.GenerateRowWithDefaults(rowsResult.Columns)
				success2, err2 := plugin.AddRow(config, input.Schema, input.StorageUnit, defaultRow)
				if err2 == nil && success2 {
					generatedRows++
				}
				continue
			}
			generatedRows++
		}

		additionalAttempts := 0
		maxAdditionalAttempts := input.RowCount * 5

		for generatedRows < input.RowCount && additionalAttempts < maxAdditionalAttempts {
			additionalAttempts++

			newRow, genErr := generator.GenerateRowDataWithConstraints(rowsResult.Columns, constraints)
			if genErr != nil {
				continue
			}

			success, addErr := plugin.AddRow(config, input.Schema, input.StorageUnit, newRow)
			if addErr != nil || !success {
				defaultRow := generator.GenerateRowWithDefaults(rowsResult.Columns)
				success2, err2 := plugin.AddRow(config, input.Schema, input.StorageUnit, defaultRow)
				if err2 == nil && success2 {
					generatedRows++
				}
				continue
			}
			generatedRows++
		}
	}

	if err != nil {
		log.Logger.WithError(err).WithField("generatedRows", generatedRows).Error("Mock data generation failed")
		return nil, fmt.Errorf("mock data generation failed: %w", err)
	}

	log.Logger.WithField("generatedRows", generatedRows).Info("Mock data generation completed successfully")
	return &model.MockDataGenerationStatus{
		AmountGenerated: generatedRows,
	}, nil
}

// Version is the resolver for the Version field.
func (r *queryResolver) Version(ctx context.Context) (string, error) {
	if env.ApplicationVersion != "" {
		return env.ApplicationVersion, nil
	}
	// Default fallback for development
	return "development", nil
}

// Profiles is the resolver for the Profiles field.
func (r *queryResolver) Profiles(ctx context.Context) ([]*model.LoginProfile, error) {
	var profiles []*model.LoginProfile
	for i, profile := range src.GetLoginProfiles() {
		profileName := src.GetLoginProfileId(i, profile)
		loginProfile := &model.LoginProfile{
			ID:                   profileName,
			Type:                 model.DatabaseType(profile.Type),
			Database:             &profile.Database,
			IsEnvironmentDefined: true,
			Source:               profile.Source,
		}
		if len(profile.Alias) > 0 {
			loginProfile.Alias = &profile.Alias
		}
		if len(profile.CustomId) > 0 {
			loginProfile.ID = profile.CustomId
		}
		profiles = append(profiles, loginProfile)
	}
	return profiles, nil
}

// Database is the resolver for the Database field.
// This resolver is used in two scenarios:
// 1. Login page: to get available databases (e.g., SQLite files) before authentication
// 2. Sidebar: to get switchable databases when already logged in
//
// For the sidebar case, we use session credentials. For login page, we fall back
// to a minimal config (works for SQLite which scans filesystem, not for MySQL which needs connection).
func (r *queryResolver) Database(ctx context.Context, typeArg string) ([]string, error) {
	plugin := src.MainEngine.Choose(engine.DatabaseType(typeArg))
	if plugin == nil {
		return nil, fmt.Errorf("unsupported database type: %s", typeArg)
	}

	var config *engine.PluginConfig

	// Try to get credentials from session (for sidebar when logged in)
	credentials := auth.GetCredentials(ctx)
	if credentials != nil && credentials.Type == typeArg {
		config = engine.NewPluginConfig(credentials)
	} else {
		// No session or type mismatch - use minimal config. works for sqlite
		config = &engine.PluginConfig{
			Credentials: &engine.Credentials{
				Type: typeArg,
			},
		}
	}

	databases, err := plugin.GetDatabases(config)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "GetDatabases",
			"database_type": typeArg,
		}).WithError(err).Error("Database operation failed")
		return nil, err
	}
	return databases, nil
}

// Schema is the resolver for the Schema field.
func (r *queryResolver) Schema(ctx context.Context) ([]string, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type
	schemas, err := plugin.GetAllSchemas(config)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "GetAllSchemas",
			"database_type": typeArg,
		}).WithError(err).Error("Database operation failed")
		return nil, err
	}
	return schemas, nil
}

// StorageUnit is the resolver for the StorageUnit field.
func (r *queryResolver) StorageUnit(ctx context.Context, schema string) ([]*model.StorageUnit, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type
	units, err := plugin.GetStorageUnits(config, schema)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "GetStorageUnits",
			"schema":        schema,
			"database_type": typeArg,
		}).WithError(err).Error("Database operation failed")
		return nil, err
	}
	var storageUnits []*model.StorageUnit
	for _, unit := range units {
		storageUnit := engine.GetStorageUnitModel(unit)
		storageUnit.IsMockDataGenerationAllowed = env.IsMockDataGenerationAllowed(unit.Name)
		storageUnits = append(storageUnits, storageUnit)
	}
	return storageUnits, nil
}

// Row is the resolver for the Row field.
func (r *queryResolver) Row(ctx context.Context, schema string, storageUnit string, where *model.WhereCondition, sort []*model.SortCondition, pageSize int, pageOffset int) (*model.RowsResult, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type

	if err := ValidateStorageUnit(plugin, config, schema, storageUnit); err != nil {
		return nil, err
	}

	// Run GetRows, GetColumnConstraints, and GetForeignKeyRelationships in parallel
	var rowsResult *engine.GetRowsResult
	var constraints map[string]map[string]any
	var foreignKeys map[string]*engine.ForeignKeyRelationship

	g, _ := errgroup.WithContext(ctx)

	g.Go(func() error {
		var err error
		rowsResult, err = plugin.GetRows(config, schema, storageUnit, where, sort, pageSize, pageOffset)
		if err != nil {
			log.LogFields(log.Fields{
				"operation":     "GetRows",
				"schema":        schema,
				"storage_unit":  storageUnit,
				"database_type": typeArg,
				"page_size":     pageSize,
				"page_offset":   pageOffset,
			}).WithError(err).Error("Database operation failed")
			return err
		}
		return nil
	})

	g.Go(func() error {
		var err error
		constraints, err = plugin.GetColumnConstraints(config, schema, storageUnit)
		if err != nil {
			log.LogFields(log.Fields{
				"operation":    "GetColumnConstraints",
				"schema":       schema,
				"storage_unit": storageUnit,
				"error":        err.Error(),
			}).Warn("Failed to get column constraints, primary key detection unavailable")
			constraints = make(map[string]map[string]any)
		}
		return nil // Non-critical, don't fail the request
	})

	g.Go(func() error {
		var err error
		foreignKeys, err = plugin.GetForeignKeyRelationships(config, schema, storageUnit)
		if err != nil {
			log.LogFields(log.Fields{
				"operation":    "GetForeignKeyRelationships",
				"schema":       schema,
				"storage_unit": storageUnit,
				"error":        err.Error(),
			}).Warn("Failed to get foreign key relationships")
			foreignKeys = make(map[string]*engine.ForeignKeyRelationship)
		}
		return nil // Non-critical, don't fail the request
	})

	if err := g.Wait(); err != nil {
		return nil, err
	}

	var columns []*model.Column
	for _, column := range rowsResult.Columns {
		isPrimary := false
		if colConstraints, ok := constraints[column.Name]; ok {
			if primary, exists := colConstraints["primary"]; exists {
				if primaryBool, isBool := primary.(bool); isBool {
					isPrimary = primaryBool
				}
			}
		}

		// Check if this column has a foreign key constraint
		var referencedTable *string
		var referencedColumn *string
		isForeignKey := false
		if fk, exists := foreignKeys[column.Name]; exists {
			isForeignKey = true
			referencedTable = &fk.ReferencedTable
			referencedColumn = &fk.ReferencedColumn
		}

		columns = append(columns, &model.Column{
			Type:             column.Type,
			Name:             column.Name,
			IsPrimary:        isPrimary,
			IsForeignKey:     isForeignKey,
			ReferencedTable:  referencedTable,
			ReferencedColumn: referencedColumn,
			Length:           column.Length,
			Precision:        column.Precision,
			Scale:            column.Scale,
		})
	}
	return &model.RowsResult{
		Columns:       columns,
		Rows:          rowsResult.Rows,
		DisableUpdate: rowsResult.DisableUpdate,
		TotalCount:    int(rowsResult.TotalCount),
	}, nil
}

// Columns is the resolver for the Columns field.
func (r *queryResolver) Columns(ctx context.Context, schema string, storageUnit string) ([]*model.Column, error) {
	plugin, config := GetPluginForContext(ctx)
	columns, err := FetchColumnsForStorageUnit(plugin, config, schema, storageUnit)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "Columns",
			"schema":        schema,
			"storage_unit":  storageUnit,
			"database_type": config.Credentials.Type,
			"error":         err.Error(),
		}).Error("Failed to fetch columns")
		return nil, err
	}
	return columns, nil
}

// ColumnsBatch is the resolver for the ColumnsBatch field.
func (r *queryResolver) ColumnsBatch(ctx context.Context, schema string, storageUnits []string) ([]*model.StorageUnitColumns, error) {
	plugin, config := GetPluginForContext(ctx)

	results := make([]*model.StorageUnitColumns, len(storageUnits))
	g, _ := errgroup.WithContext(ctx)

	for i, storageUnit := range storageUnits {
		i, storageUnit := i, storageUnit
		g.Go(func() error {
			columns, err := FetchColumnsForStorageUnit(plugin, config, schema, storageUnit)
			if err != nil {
				log.LogFields(log.Fields{
					"operation":     "ColumnsBatch",
					"schema":        schema,
					"storage_unit":  storageUnit,
					"database_type": config.Credentials.Type,
					"error":         err.Error(),
				}).Error("Failed to fetch columns")
				return err
			}
			results[i] = &model.StorageUnitColumns{
				StorageUnit: storageUnit,
				Columns:     columns,
			}
			return nil
		})
	}

	if err := g.Wait(); err != nil {
		return nil, err
	}

	return results, nil
}

// RawExecute is the resolver for the RawExecute field.
func (r *queryResolver) RawExecute(ctx context.Context, query string) (*model.RowsResult, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type
	rowsResult, err := plugin.RawExecute(config, query)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "RawExecute",
			"database_type": typeArg,
			"query":         query,
		}).WithError(err).Error("Database operation failed")
		return nil, err
	}
	var columns []*model.Column
	for _, column := range rowsResult.Columns {
		columns = append(columns, &model.Column{
			Type:         column.Type,
			Name:         column.Name,
			IsPrimary:    column.IsPrimary,
			IsForeignKey: column.IsForeignKey,
		})
	}
	return &model.RowsResult{
		Columns: columns,
		Rows:    rowsResult.Rows,
	}, nil
}

// Graph is the resolver for the Graph field.
func (r *queryResolver) Graph(ctx context.Context, schema string) ([]*model.GraphUnit, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type
	graphUnits, err := plugin.GetGraph(config, schema)
	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "GetGraph",
			"schema":        schema,
			"database_type": typeArg,
		}).WithError(err).Error("Database operation failed")
		return nil, err
	}
	var graphUnitsModel []*model.GraphUnit
	for _, graphUnit := range graphUnits {
		var relations []*model.GraphUnitRelationship
		for _, relation := range graphUnit.Relations {
			relations = append(relations, &model.GraphUnitRelationship{
				Name:         relation.Name,
				Relationship: model.GraphUnitRelationshipType(relation.RelationshipType),
				SourceColumn: relation.SourceColumn,
				TargetColumn: relation.TargetColumn,
			})
		}
		graphUnitsModel = append(graphUnitsModel, &model.GraphUnit{
			Unit:      engine.GetStorageUnitModel(graphUnit.Unit),
			Relations: relations,
		})
	}
	return graphUnitsModel, nil
}

// AIProviders is the resolver for the AIProviders field.
func (r *queryResolver) AIProviders(ctx context.Context) ([]*model.AIProvider, error) {
	providers := env.GetConfiguredChatProviders()
	var aiProviders []*model.AIProvider
	for _, provider := range providers {
		aiProviders = append(aiProviders, &model.AIProvider{
			Type:                 provider.Type,
			ProviderID:           provider.ProviderId,
			IsEnvironmentDefined: true,
		})
	}
	return aiProviders, nil
}

// AIModel is the resolver for the AIModel field.
func (r *queryResolver) AIModel(ctx context.Context, providerID *string, modelType string, token *string) ([]string, error) {
	config := engine.NewPluginConfig(auth.GetCredentials(ctx))

	// Initialize ExternalModel to prevent nil pointer dereference
	config.ExternalModel = &engine.ExternalModel{
		Type: modelType,
	}

	if providerID != nil {
		providers := env.GetConfiguredChatProviders()
		for _, provider := range providers {
			if provider.ProviderId == *providerID {
				config.ExternalModel.Token = provider.APIKey
				break
			}
		}
	} else if token != nil {
		config.ExternalModel.Token = *token
	}
	models, err := llm.Instance(config).GetSupportedModels()
	if err != nil {
		log.LogFields(log.Fields{
			"operation":   "GetSupportedModels",
			"model_type":  modelType,
			"provider_id": providerID,
			"error":       err.Error(),
		}).Error("AI operation failed")
		return nil, err
	}
	return models, nil
}

// AIChat is the resolver for the AIChat field.
func (r *queryResolver) AIChat(ctx context.Context, providerID *string, modelType string, token *string, schema string, input model.ChatInput) ([]*model.AIChatMessage, error) {
	plugin, config := GetPluginForContext(ctx)
	typeArg := config.Credentials.Type
	if providerID != nil {
		providers := env.GetConfiguredChatProviders()
		for _, provider := range providers {
			if provider.ProviderId == *providerID {
				config.ExternalModel = &engine.ExternalModel{
					Type:  modelType,
					Token: provider.APIKey,
				}
			}
		}
	} else {
		config.ExternalModel = &engine.ExternalModel{
			Type: modelType,
		}
		if token != nil {
			config.ExternalModel.Token = *token
		}
	}
	messages, err := plugin.Chat(config, schema, input.Model, input.PreviousConversation, input.Query)

	if err != nil {
		log.LogFields(log.Fields{
			"operation":     "Chat",
			"schema":        schema,
			"database_type": typeArg,
			"model":         input.Model,
			"model_type":    modelType,
			"provider_id":   providerID,
			"query":         input.Query,
			"error":         err.Error(),
		}).Error("AI chat operation failed")
		return nil, err
	}

	var chatResponse []*model.AIChatMessage

	for _, message := range messages {
		var result *model.RowsResult
		if strings.HasPrefix(message.Type, "sql") {
			var columns []*model.Column
			for _, column := range message.Result.Columns {
				columns = append(columns, &model.Column{
					Type: column.Type,
					Name: column.Name,
				})
			}
			result = &model.RowsResult{
				Columns: columns,
				Rows:    message.Result.Rows,
			}
		}
		chatResponse = append(chatResponse, &model.AIChatMessage{
			Type:   message.Type,
			Result: result,
			Text:   message.Text,
		})
	}

	return chatResponse, nil
}

// SettingsConfig is the resolver for the SettingsConfig field.
func (r *queryResolver) SettingsConfig(ctx context.Context) (*model.SettingsConfig, error) {
	currentSettings := settings.Get()
	return &model.SettingsConfig{MetricsEnabled: &currentSettings.MetricsEnabled}, nil
}

// MockDataMaxRowCount is the resolver for the MockDataMaxRowCount field.
func (r *queryResolver) MockDataMaxRowCount(ctx context.Context) (int, error) {
	return env.GetMockDataGenerationMaxRowCount(), nil
}

// DatabaseMetadata is the resolver for the DatabaseMetadata field.
func (r *queryResolver) DatabaseMetadata(ctx context.Context) (*model.DatabaseMetadata, error) {
	plugin, _ := GetPluginForContext(ctx)
	if plugin == nil {
		return nil, nil
	}
	metadata := plugin.GetDatabaseMetadata()

	// Return nil if plugin doesn't implement metadata (default GormPlugin behavior)
	if metadata == nil {
		return nil, nil
	}

	// Convert engine.TypeDefinition to model.TypeDefinition
	typeDefinitions := make([]*model.TypeDefinition, 0, len(metadata.TypeDefinitions))
	for _, td := range metadata.TypeDefinitions {
		typeDefinitions = append(typeDefinitions, &model.TypeDefinition{
			ID:               td.ID,
			Label:            td.Label,
			HasLength:        td.HasLength,
			HasPrecision:     td.HasPrecision,
			DefaultLength:    td.DefaultLength,
			DefaultPrecision: td.DefaultPrecision,
			Category:         model.TypeCategory(td.Category),
		})
	}

	// Convert map[string]string to []*model.Record
	aliasMap := make([]*model.Record, 0, len(metadata.AliasMap))
	for key, value := range metadata.AliasMap {
		aliasMap = append(aliasMap, &model.Record{
			Key:   key,
			Value: value,
		})
	}

	return &model.DatabaseMetadata{
		DatabaseType:    string(metadata.DatabaseType),
		TypeDefinitions: typeDefinitions,
		Operators:       metadata.Operators,
		AliasMap:        aliasMap,
	}, nil
}

// Mutation returns MutationResolver implementation.
func (r *Resolver) Mutation() MutationResolver { return &mutationResolver{r} }

// Query returns QueryResolver implementation.
func (r *Resolver) Query() QueryResolver { return &queryResolver{r} }

type mutationResolver struct{ *Resolver }
type queryResolver struct{ *Resolver }
